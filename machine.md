# cnn
卷积是数学的一种运算，
<img src = "https://img-blog.csdnimg.cn/20201111165219454.png">
<img src="https://img-blog.csdnimg.cn/img_convert/371436a5a3d445749628cc963d8a7357.gif">
卷积神经网络的核心是用卷积核来提取数据特征

## 卷积层
-----------
卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。
<img src="https://cuijiahua.com/wp-content/uploads/2018/01/dl_3_12.gif">

## 激活函数层
-----------
卷积层做的是特正提取，但是无法描述复杂空间（非线性），引入激活函数提高复杂空间描述能力

## 池化层
-----------
用以降低参数数量，提升速度，增加感受视野。是降维操作，提升了容错能力，防止国拟合
<img src="https://cuijiahua.com/wp-content/uploads/2018/01/dl_3_3.png">
mean-pooling能更多的保留图像的背景信息，更加关注背景，一般使用在网络结尾。

max-pooling能更多的保留图像的纹理信息，更加关注前景，一般使用在网络层中。

随机池化中元素值大的被选中的概率也大，但不是像最大池化总是取最大值。随机池化一方面最大化地保证了Max值的取值，一方面又确保了不会完全是max值起作用，造成过度失真。除此之外，其可以在一定程度上避免过拟合。

## 扩张卷积，又称空洞卷积
-----------------
dilation， 扩大卷积核的视野，3x3 的卷积核的视野可以不是3x3,如果dilation=2, 那么3x3核的视野就是5x5了
## Dropout, 解决过拟合
------------
思想：过拟合就是所有参数（神经元）都发挥着独到的作用，那么把某些神经元以概率p进行剔除，让神经元更加常规（泛化）

1. 多模型平均， 每次提出的神经元不一样
2. 减少神经元之间的依赖，对特定的特征不敏感
3. 生物进化，每次剔除的不一样，重要的神经元会被留下来赋予更高的全重
## BN 层
----------
层数多，难于收敛。
直接对特征进行白化操作=去均值方差, 这样所有数据就有一样的均值和方差了
<img src="https://img-blog.csdnimg.cn/img_convert/627a92c6f2ec557f950c8fb35264629d.png">

1. 缓解梯度消失，加速收敛
2. 简化调参，网络更稳定；
3. 防止国拟合

缺点：
1. 需要的很大的batch
2. 训练集和测试集的样本分布有时候不一样

## 全连接层
----------
卷积层作的是特征提取， 全连接作的是特征映射到标签空间。

# RNN LSTM

rnn更容易梯度爆炸或消失是因为他不仅要链式求导数还要横向求导数（单词之间有依赖）

LSTM（https://blog.csdn.net/qian99/article/details/88628383）

RNN适合作short term 模式，