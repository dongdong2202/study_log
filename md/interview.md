## 线性回归要求数据是正太分布？
因为噪声通常是正太分布的，数据只有是正太分别拟合效果才更好；
sigmoid将R映射到【-1, 1】

## 逻辑斯特回归为什么要对特征进行离散化。

    速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
    鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
    方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
## 决策树， 简单、可解释性好、逻辑清晰
-----------
<img src="https://camo.githubusercontent.com/f8806abaad252a66833e412e9bf8140528a185e81aeea31de66014e94e729312/68747470733a2f2f7778322e73696e61696d672e636e2f6c617267652f30303633304465666c7931673471323836766969626a3330706b3070666b30392e6a7067">

这颗“树”长到什么时候停

    当前结点包含的样本全属于同一类别，无需划分；例如：样本当中都是决定去相亲的，属于同一类别，就是不管特征如何改变都不会影响结果，这种就不需要划分了。
    当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。
    当前结点包含的样本集合为空，不能划分。

引入了一个概念就是纯度，想想也是如此，大众选择就意味着纯度越高。好，在深入一点，就涉及到一句话：信息熵越低，纯度越高。我相信大家或多或少都听说过“熵”这个概念，信息熵通俗来说就是用来度量包含的“信息量”，如果样本的属性都是一样的，就会让人觉得这包含的信息很单一，没有差异化，相反样本的属性都不一样，那么包含的信息量就很多了。

决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。

    预剪枝：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。
    后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。

工程上总的来说：
CART和C4.5之间主要差异在于分类结果上，CART可以回归分析也可以分类，C4.5只能做分类；C4.5子节点是可以多分的，而CART是无数个二叉子节点；
以此拓展出以CART为基础的“树群”random forest ， 以回归树为基础的“树群”GBDT

 

样本数据的差异：
ID3只能对分类变量进行处理，C4.5和CART可以处理连续和分类两种自变量
ID3对缺失值敏感，而C4.5和CART对缺失值可以进行多种方式的处理
只从样本量考虑，小样本建议考虑c4.5、大样本建议考虑cart。c4.5处理过程中需对数据集进行多次排序，处理成本耗时较高，而cart本身是一种大样本的统计方法，小样本处理下泛化误差较大

目标因变量的差异：
ID3和C4.5只能做分类，CART（分类回归树）不仅可以做分类（0/1）还可以做回归（0-1）
ID3和C4.5节点上可以产出多叉（低、中、高），而CART节点上永远是二叉（低、非低）

样本特征上的差异：
特征变量的使用中，多分的分类变量ID3和C4.5层级之间只单次使用，CART可多次重复使用

决策树产生过程中的优化差异：
C4.5是通过枝剪来修正树的准确性，而CART是直接利用全部数据发现所有树的结构进行对比


## 随机森林有什么优缺点

优点：

    在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。
    它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。
    在训练完后，它能够给出哪些feature比较重要。
    训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。
    在训练过程中，能够检测到feature间的互相影响。
    对于不平衡的数据集来说，它可以平衡误差。
    如果有很大一部分的特征遗失，仍可以维持准确度。

缺点：

    随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
    对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

计算袋外错误率oob error（out-of-bag error）。

## GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是Boosting的思想。
-----------
Boosting思想

Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。

Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。

 GBDT的优点和局限性有哪些？
3.1 优点

    预测阶段的计算速度快，树与树之间可并行化计算。
    在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
    采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。

3.2 局限性

    GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
    GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
    训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。
4. RF(随机森林)与GBDT之间的区别与联系

相同点：

    都是由多棵树组成，最终的结果都是由多棵树一起决定。
    RF和GBDT在使用CART树时，可以是分类树或者回归树。

不同点：

    组成随机森林的树可以并行生成，而GBDT是串行生成
    随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
    随机森林对异常值不敏感，而GBDT对异常值比较敏感
    随机森林是减少模型的方差，而GBDT是减少模型的偏差
    随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

## SVG
-----------
4. 线性分类器与非线性分类器的区别以及优劣

线性和非线性是针对模型参数和输入特征来讲的；比如输入x，模型y=ax+ax^2 那么就是非线性模型，如果输入是x和X^2则模型是线性的。

    线性分类器可解释性好，计算复杂度较低，不足之处是模型的拟合效果相对弱些。

    LR,贝叶斯分类，单层感知机、线性回归

    非线性分类器效果拟合能力较强，不足之处是数据量不足容易过拟合、计算复杂度高、可解释性不好。

    决策树、RF、GBDT、多层感知机

LR是参数模型，svm是非参数模型，

1.1 判别式模型这么做：

根据训练数据得到分类函数和分界面，比如说根据SVM模型得到一个分界面，然后直接计算条件概率 [公式] ，我们将最大的 [公式] 作为新样本的分类。判别式模型是对条件概率建模，学习不同类别之间的最优边界，无法反映训练数据本身的特性，能力有限，其只能告诉我们分类的类别。

1.2 生成式模型这么做

一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。比如说类别标签有｛猫，狗，猪｝，那首先根据猫的特征学习出一个猫的模型，再根据狗的特征学习出狗的模型，之后分别计算新样本 [公式] 跟三个类别的联合概率 [公式] ，然后根据贝叶斯公式：

## KNN 	K-Means GMM
--------------
    1.KNN是分类算法
    2.属于监督学习
    3.训练数据集是带label的数据 
K的含义：一个样本x，对它进行分类，就从训练数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c。

    1.K-Means是聚类算法
    2.属于非监督学习
    3.训练数据集是无label的数据，是杂乱无章的，经过聚类后变得有序，先无序，后有序。
K的含义：K是人工固定好的数字，假设数据集合可以分为K个蔟，那么就利用训练数据来训练出这K个分类。

GMM与K-Means相比

高斯混合模型与K均值算法的相同点是：

    它们都是可用于聚类的算法；
    都需要 指定K值；
    都是使用EM算法来求解；
    都往往只能收敛于局部最优。

而它相比于K 均值算法的优点是，可以给出一个样本属于某类的概率是多少；不仅仅可以用于聚类，还可以用于概率密度的估计；并且可以用于生成新的样本点。


用官方的话来说，所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

KNN算法作为一个最简单，也是一个很实用的机器学习的算法，日常的使用中也能处理很多问题，这里做一下总结记录

优点

1、KNN可以处理分类问题，同时天然可以处理多分类问题，比如鸢尾花的分类

2、简单，易懂，同时也很强大，对于手写数字的识别，鸢尾花这一类问题来说，准确率很高

3、KNN还可以处理回归问题，也就是预测




（1）优点

    ①简单，易于理解，易于实现，无需参数估计，无需训练;
    ②精度高，对异常值不敏感（个别噪音数据对结果的影响不是很大）;
    ③适合对稀有事件进行分类;
    ④特别适合于多分类问题(multi-modal,对象具有多个类别标签)，KNN要比SVM表现要好.

knn缺点

    1、效率低，因为每一次分类或者回归，都要把训练数据和测试数据都算一遍，如果数据量很大的话，需要的算力会很惊人，但是在机器学习中，大数据处理又是很常见的一件事

    2、对训练数据依赖度特别大，虽然所有机器学习的算法对数据的依赖度很高，但是KNN尤其严重，因为如果我们的训练数据集中，有一两个数据是错误的，刚刚好又在我们需要分类的数值的旁边，这样就会直接导致预测的数据的不准确，对训练数据的容错性太差

    3、维数灾难，KNN对于多维度的数据处理也不是很好，如下图

## 特征工程


线性函数归一化（Min-Max Scaling）。它对原始数据进行线性变换，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中X为原始数据， 分别为数据最大值和最小值。

x = x-xmin/xmax-xmin

零均值归一化（Z-Score Normalization）。它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么 归一化公式定义为

x = x -u/tao

## LDA和PCA区别
异同点 	LDA 	PCA
相同点 	1. 两者均可以对数据进行降维；
2. 两者在降维时均使用了矩阵特征分解的思想；
3. 两者都假设数据符合高斯分布； 	
不同点 	有监督的降维方法； 	无监督的降维方法；
	降维最多降到k-1维； 	降维多少没有限制；
	可以用于降维，还可以用于分类； 	只用于降维；
	选择分类性能最好的投影方向； 	选择样本点投影具有最大方差的方向；
	更明确，更能反映样本间差异； 	目的较为模糊；


# 什么是迁移学习

迁移学习(Transfer Learning)是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程#

## 为什么需要迁移学习？

    大数据与少标注的矛盾：虽然有大量的数据，但往往都是没有标注的，无法训练机器学习模型。人工进行数据标定太耗时。
    大数据与弱计算的矛盾：普通人无法拥有庞大的数据量与计算资源。因此需要借助于模型的迁移。
    普适化模型与个性化需求的矛盾：即使是在同一个任务上，一个模型也往往难以满足每个人的个性化需求，比如特定的隐私设置。这就需要在不同人之间做模型的适配
8. 什么情况下可以使用迁移学习？

迁移学习最有用的场合是，如果你尝试优化任务B的性能，通常这个任务数据相对较少。 例如，在放射科中你知道很难收集很多射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用 1 百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务在放射科任务上做得更好，尽管任务没有这么多数据。

什么是finetune？

度网络的finetune也许是最简单的深度网络迁移方法。Finetune,也叫微调、fine-tuning, 是深度学习中的一个重要概念。简而言之，finetune就是利用别人己经训练好的网络，针对自己的任务再进行调整。从这个意思上看，我们不难理解finetune是迁移学习的一部分。

什么是深度网络自适应？

深度网络的 finetune 可以帮助我们节省训练时间，提高学习精度。但是 finetune 有它的先天不足:它无法处理训练数据和测试数据分布不同的情况。而这一现象在实际应用中比比皆是。因为 finetune 的基本假设也是训练数据和测试数据服从相同的数据分布。这在迁移学习中也是不成立的。因此，我们需要更进一步，针对深度网络开发出更好的方法使之更好地完成迁移学习任务。

以我们之前介绍过的数据分布自适应方法为参考，许多深度学习方法都开发出了自适应层(AdaptationLayer)来完成源域和目标域数据的自适应。自适应能够使得源域和目标域的数据分布更加接近，从而使得网络的效果更好。

# GANs 或 Generative Adversarial Networks 是一类机器学习技术，由两个网络组成，相互进行对抗性学习。****

# 什么是强化学习

其他许多机器学习算法中学习器都是学得怎样做，而强化学习（Reinforcement Learning, RL）是在尝试的过程中学习到在特定的情境下选择哪种行动可以得到最大的回报。在很多场景中，当前的行动不仅会影响当前的rewards，还会影响之后的状态和一系列的rewards

##     训练误差：**模型在训练数据集上表现出的误差。
    
**泛化误差：**模型在任意⼀个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。

训练误差的期望小于或等于泛化误差。也就是说，⼀般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于⽆法从训练误差估计泛化误差，⼀味地降低训练误差并不意味着泛化误差⼀定会降低。

## 解决梯度消失or爆炸
正则
k折交叉验证
dropout
残差
激活函数

 ## 数据集小怎么办
 1. k折交叉验证
 2. 数据集增强，造数据
 3. 
1. 下采样

例如0样本数据量大约28万，1样本数据量大约几百个。可以让0样本采集几百个，跟1样本的数量同样少。
2. 上采样

生成1样本数据，与0样本的数据量一样。



# nlp

   bog-of-words 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
    词与词之间是没有顺序关系的。
tf-idf 缺点：**还是没有把词与词之间的关系顺序表达出来。

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。改模型考虑了词的顺序。

缺点：**随着n的大小增加，词表会成指数型膨胀，会越来越大。
当n=2时候， M**2个向量，爆炸了，n=5, M**5

Word2Vec存在的问题

    对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息。
    对多义词无法很好的表示和处理，因为使用了唯一的词向量


    图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。
    都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。

之前一直不明白fasttext用层次softmax时叶子节点是啥，CBOW很清楚，它的叶子节点是词和词频，后来看了源码才知道，其实fasttext叶子节点里是类标和类标的频数。
	Word2Vec 	fastText
输入 	one-hot形式的单词的向量 	embedding过的单词的词向量和n-gram向量
输出 	对应的是每一个term,计算某term概率最大 	对应的是分类的标签。

本质不同，体现在softmax的使用：

word2vec的目的是得到词向量，该词向量最终是在输入层得到的，输出层对应的h-softmax也会生成一系列的向量，但是最终都被抛弃，不会使用。

fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label

fastText优点：

    适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”
    支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText的性能要比时下流行的word2vec工具明显好上不少，也比其他目前最先进的词态词汇表征要好。
    专注于文本分类，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。

而word2vec最大的缺点则是没有充分利用所有的语料，所以GloVe其实是把两者的优点结合了起来。从这篇论文给出的实验结果来看，GloVe的性能是远超LSA和word2vec的，但网上也有人说GloVe和word2vec实际表现其实差不多。



# ks, t, f,     grubbs 检验，
# 假设性检验
思想：是出反常必有妖
在当前声称下，现实观察到的事件发生的概率有多小。

显著水平，一般=0.05,是一个经验值
p-alue，p值， 观测到的数据在声称下的概率
## 参数检验：数据成正太分布
探究连续变量之间的差异是否显著
1. t检验：数据小于100条，成正态分布， 2个样本以下，使用t
2. z检验：数据大约100条，成正态分布

## 卡方检验：
探索离散变量之间的差异性（百分比是离散的）
数据成正太分布， 一般可作独立性检验
观察数据-期望数据的差距
 
 1. grubbs 检验是定义异常值的方法
## 非参数连续数据检验，数据不需要正态分布

t检验是检查两组均值的差异
而F检验是检查多组均值之间的差异 

## 方差分析，探究离散数据和连续数据之间的差异

1. 连续数据成正太分布
2. 必须通过方差齐性分析

